# Midas

# [****Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer****]([https://arxiv.org/abs/1907.01341](https://arxiv.org/abs/1907.01341))

paper: [https://arxiv.org/abs/1907.01341](https://arxiv.org/abs/1907.01341)

code: [https://pytorch.org/hub/intelisl_midas_v2/](https://pytorch.org/hub/intelisl_midas_v2/)

### Notes:

- Monocular Depth Estimation
- A good paper of depth estimation dataset

### Abstract:

- The success of monocular depth estimation relies on large and diverse training sets
- We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible.
- we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks.
- Our losses enable training on data that was acquired with diverse sensing modalities such as stereo cameras (with potentially unknown calibration), laser scanners, and structured light sensors.
- We also quantify the value of a variety of existing datasets for monocular depth estimation and explore optimal strategies for mixing datasets during training.

### Process:

![Untitled](Midas%2004f6961537cb45ee80f8bae91f4b4847/Untitled.png)

To complement the existing datasets we propose a new data source: 3D movies (MV). Their full potential is unlocked by combining them with other, complementary data sources.

Different dataset GT form:

![Untitled](Midas%2004f6961537cb45ee80f8bae91f4b4847/Untitled%201.png)

! Cause the research direction is not on the depth estimation, I did read so deeply of this literature. More details could be checked from the original paper. 

### Result:

![Untitled](Midas%2004f6961537cb45ee80f8bae91f4b4847/Untitled%202.png)

### Conclusion:

The success of deep networks has been driven by massive datasets. For monocular depth estimation, we believe that existing datasets are still insufficient and likely constitute the limiting factor. Motivated by the difficulty of capturing diverse depth datasets at scale, we have introduced tools for combining complementary sources of data. We have proposed a flexible loss function and a principled dataset mixing strategy. We have further introduced a dataset based on 3D movies that provides dense ground truth for diverse dynamic scenes. We have evaluated the robustness and generality of models via zero-shot cross-dataset transfer. We find that systematically testing models on datasets that were never seen during training is a better proxy for their performance “in the wild” than testing on a heldout portion of even the most diverse datasets that are currently available. Our work advances the state of the art in generic monocular depth estimation and indicates that the presented ideas substantially improve performance across diverse environments. We hope that this work will contribute to the deployment of monocular depth models that meet the requirements of practical applications. Our models are freely available at [https://github.com/intel-isl/MiDaS](https://github.com/intel-isl/MiDaS).