# DINOv2

Paper: [https://arxiv.org/abs/2304.07193](https://arxiv.org/abs/2304.07193)

Demo: [https://dinov2.metademolab.com/](https://dinov2.metademolab.com/)

## [DINOv2: Learning Robust Visual Features without Supervision]([https://arxiv.org/abs/2304.07193](https://arxiv.org/abs/2304.07193))

### Note:

- Foundation model with big data in the downstream problem is better than the task-specfic models
- For the foundation model, it indeed needs the the data with no label and no supervision(but it is easy for the text model—> just use the mask), but for the image it is a bit hard…
- The text-guided pretraining limits the information that can be retained about the image since captions only approximate the rich information in images not complex pixel-level information. Furthermore, these image encoders require aligned text-image corpora and hence, do not offer the flexibility of their text counterparts, that is, to learn from raw data alone
- The data quality and diversity are essential to produce good features

### Abstract:

- This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources.
- features that work across image distributions and tasks **without finetuning**
- we propose an **automatic pipeline** to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature.

### Process:

- **Use PCA to extact the feature is interesting and also useful:**

![Untitled](DINOv2%20a8f7696f02984e0cb3ca09bda6839177/Untitled.png)

- **Data processing pipeline:**

![Untitled](DINOv2%20a8f7696f02984e0cb3ca09bda6839177/Untitled%201.png)

- **Discriminative Self-supervised Pre-training (Pre-train step):**

We learn our features with a discriminative self-supervised method that can be seen as a combination of DINO and iBOT losses with the centering of SwAV (Caron et al., 2020). We also add a regularizer to spread features and a short high-resolution training phase.

- **Image-level objective:** the cross-entropy loss between the features extracted from a student and a teacher network, we learn the parameters of the student and build the teacher with an exponential moving average of past iterates
- **Patch-level objective**: We randomly mask some of the input patches given to the student, but not to the teacher. We then add a cross-entropy loss between the patch features of both networks on each masked patch.
- **Untying head weights between both objectives:** We observe that tying the weights associated with both objectives makes the model underfit at the patch-level while overfitting at the image-level. Untying these weights resolves this issue and improve the performances at both scales
- **Sinkhorn-Knopp centering:** Replace the teacher softmax-centering step of DINO and iBot by the Sinkhorn-Knopp (SK) batch normalization of SwAV (Caron et al., 2020). We run the Sinkhorn-Knopp algorithm steps for 3 iterations. For the student, we apply the softmax normalization.
- **KoLeo regularizer:** The KoLeo regularizer derives from the Kozachenko-Leonenko differential entropy estimator and encourages a uniform span of the features within a batch.
- **Adapting the resolution:** Increasing image resolution is key to pixellevel downstream tasks such as segmentation or detection, where small objects disappear at low resolutions. However, training at high resolution is time and memory demanding, and instead, we increase the resolution of images to 518 × 518 during a short period at the end of pretraining.

### Result:

- **Baseline:**
    
    Baselines. In our comparisons, we use two kinds of models as baselines. We compare to the best performing **self-supervised** models that are openly available. First, we run our evaluations for **MAE** (He et al., 2021), **DINO** (Caron et al., 2021), **SEERv2** (Goyal et al., 2022a), **MSN** (Assran et al., 2022), **EsViT** (Li et al., 2022a), **Mugs** (Zhou et al., 2022) and **iBOT** (Zhou et al., 2021). When several architectural variants were proposed for a given method, we report results for the one that leads to best top-1 accuracy on ImageNet-1k. Second, we report performance of open-source **weakly-supervised** models such as **CLIP** (Radford et al., 2021), **OpenCLIP** (Ilharco et al., 2021), and **SWAG** (Singh et al., 2022). When evaluating models on ImageNet-1k, we report the performance for each of the aforementioned methods. For all other evaluations, we report the four best-performing models amongst SSL ones. Also, for reference, we report the best performing OpenCLIP-G for weakly-supervised ones.
    
    ![Untitled](DINOv2%20a8f7696f02984e0cb3ca09bda6839177/Untitled%202.png)
    

### Conclusion:

In this work, we present DINOv2, a new series of image encoders pretrained on large curated data with no supervision. This is the first SSL work on image data that leads to visual features that close the performance gap with (weakly) supervised alternatives across a wide range of benchmarks and without the need for finetuning. A few properties emerge from these models, such as an understanding of object parts and scene geometry regardless of the image domains. We expect that more of these properties will emerge at larger scales of models and data, akin to instruction emergence in large language models, and plan to continue scaling along these axes. This paper also demonstrates that these visual features are compatible with classifiers as simple as linear layers - meaning the underlying information is readily available. In future work, we plan to leverage this ability to train a a language-enabled AI system that can process visual features as if they were word tokens, and extract the required information to ground the system.