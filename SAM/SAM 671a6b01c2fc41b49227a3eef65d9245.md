# SAM

Link: [https://segment-anything.com/](https://segment-anything.com/)

Paper: [https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)

Code: [https://github.com/facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything)

## [Segment Anything]([https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf))

### Note:

- Personally speaking: The future of the CV is the big foundational model.
- Foundation models are a promising development that can perform zero-shot and few-shot learning → foundation model for different task but not easy for small company

### Abstract:

- with over 1 billion masks on 11M licensed and privacy respecting images
- The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks
- The success of this plan hinges on three components: task, model, and data

### Process:

Three Keys:

![Untitled](SAM%20671a6b01c2fc41b49227a3eef65d9245/Untitled.png)

- Task:
    
    Promptable segmentation task, even when a prompt is ambiguous and could refer to multiple objects , the output should be a reasonable mask for at least one of those objects, ex:
    
    ![Untitled](SAM%20671a6b01c2fc41b49227a3eef65d9245/Untitled%201.png)
    
- Model:
    
    The model must support flexible prompts, needs to compute masks in amortized real-time to allow interactive use, and must be ambiguity-aware → a simple design satisfies all three constraints: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and then the two information sources are combined in a lightweight mask decoder
    
    To make SAM ambiguity-aware, we design it to predict multiple masks for a single prompt allowing SAM to naturally handle ambiguity
    
    Loss: linear combination of focal loss and dice loss
    

![Untitled](SAM%20671a6b01c2fc41b49227a3eef65d9245/Untitled%202.png)

- Data:
    
    To achieve strong generalization to new data distributions, it necessary to train SAM on a large and diverse set of masks. → too large → we co-develop our model with model-in-the-loop dataset
    
    - Data Engine
        
        has three stages: 
        
        assisted-manual: classical segment annotating
        
        semi-automatic: with prompts and manual annotation for the rest
        
        fully automatic: prompt SAM with a regular grid of foreground points
        

![Untitled](SAM%20671a6b01c2fc41b49227a3eef65d9245/Untitled%203.png)

The sample result is very impressive, but when I try the demo, the result is not as excellent, but still good enough. Maybe it needs manual correction.

### Result:

![Untitled](SAM%20671a6b01c2fc41b49227a3eef65d9245/Untitled%204.png)

![Untitled](SAM%20671a6b01c2fc41b49227a3eef65d9245/Untitled%205.png)

Zero-shot has more details! → Impressive!

### Conclusion:

- **Foundation Models:**
    
    Foundation model → models that are “trained on broad data at scale and are adaptable to a wide range of downstream tasks(specifc task), but SAM is just a important subset of computer vision
    
- **Compositionality:**
    
    We aim to achieve this by requiring SAM to predict a valid mask for a wide range of segmentation prompts. For example, MCC can easily use SAM to segment an object of interest and achieve strong generalization to unseen objects for 3D reconstruction from a single RGB-D image
    
- **Limitations:**
    
    It can miss fine structures, hallucinates small disconnected components at times, and does not produce boundaries as crisply as more computationally intensive methods that “zoom-in”. SAM is designed for generality and breadth of use rather than high IoU interactive segmentation.
    
- **Conclusion:**
    
    The Segment Anything project is an attempt to lift image segmentation into the era of foundation models